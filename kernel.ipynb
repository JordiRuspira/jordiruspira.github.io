{
  "cells": [
    {
      "metadata": {
        "_uuid": "5a4f92c87fcad37702d5cd771d52e913302928be"
      },
      "cell_type": "markdown",
      "source": "# PASSNYC Exploration | Data for Good\n\n**This code is based on previous works and it's still under construction**\n\n## Table of contents\n\n* Introduction\n* Data pre-processing\n* Data visualization\n* Conclusion"
    },
    {
      "metadata": {
        "_uuid": "cc0fe12d94c95c778b12de4e75b0b117d682ba5a"
      },
      "cell_type": "markdown",
      "source": "## Introduction\n\nBefore we start, we will to try to understand our data. We have two datasets: \"2016 School Explorer\" (from now on **df**) and \"D5 SHSAT Registrations and Testers\" (from now on **registrations**). "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('../input/2016 School Explorer.csv')\nregistrations = pd.read_csv('../input/D5 SHSAT Registrations and Testers.csv')\n\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('----- Dataframe Information -----')\nprint('---------------------------------')\nprint(df.info())\nprint('----- Registrations Information -----')\nprint('-------------------------------------')\nprint(registrations.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "01af2352e7c8c5e5c64a1b6c6c9e659785792c4e"
      },
      "cell_type": "markdown",
      "source": "The dataframe *df* has a total of 161 columns of different types, most of them integers. Let's take a deeper look into each column. On the other hand, the *registrations* dataframe has a total of 7 columns. Let's get into more detail, starting with *df*."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2898dd98663cbdb210eb5833c89ddee43f70c155",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for i in df.columns:\n    print(i,',' ,'{:.1%}'.format(np.mean(df[i].isnull())),'nulls',',',type(df[i][0]), \n    df[i].nunique(), 'unique values')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bbe1261a98722618ce459ea18fa0b45152909d44"
      },
      "cell_type": "markdown",
      "source": "We see that there are a few columns with a crazy amount of null elements, so we will get rid of those columns (\"Adjusted grade\", \"New?\", \"Other Location Code in LCGMS\"). At some point we will also deal with the column \"School Income Estimate\", which has a 30% of null elements but gives a relevant information."
    },
    {
      "metadata": {
        "_uuid": "221dbeab128a1b16a9ecbb71a7213a2cfde08a9a"
      },
      "cell_type": "markdown",
      "source": "How can we fill the missing values in the column? The most common thing to do is taking the mean or median, depending on the type of data, within a certain range. We will make a correlation map between this column and others, to see where we can take the mean."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ace5101f8a4ea8f317702ee6d3ab34e872e8224",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df['School Income Estimate'] = df['School Income Estimate'].str.replace('$', '')\ndf['School Income Estimate'] = df['School Income Estimate'].str.replace(',', '')\ndf['School Income Estimate'] = df['School Income Estimate'].str.replace('.', '')\ndf['School Income Estimate'] = df['School Income Estimate'].astype(float)\n\ndf = df.drop(columns=['Adjusted Grade', 'New?', 'Other Location Code in LCGMS'])\ncorrcolumns = ['SED Code', 'District', 'Latitude', 'Longitude', 'Zip', 'Economic Need Index'] # Community School? would be interesting aswell\nfor i in corrcolumns:\n    print('The correlation value between School Income Estimate and ', i, 'is: ', df['School Income Estimate'].corr(df[i]))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b3108a24b256c883d54ce7858c9e810f322aa9a"
      },
      "cell_type": "markdown",
      "source": "Taking only the previous examples, we can see that **school income estimate** and **economic need index** are almost completely anti-correlated. That makes sense; if you have an high income, you will have a small economic need (index), and viceversa. Thus, this can be a good aspect to analyse, and we will use it when it comes to data pre-processing."
    },
    {
      "metadata": {
        "_uuid": "57a73446c703718d1021cedabfda9affd7232abf"
      },
      "cell_type": "markdown",
      "source": "# Registrations\n\nBefore we go into detail of the dataframe, let's do a recap. So what exactly is the SHSAT exam? There are **nine** specialized High Schools in New York, which offer a more rigorous curriculum than most other public high schools in the city. The schools are intended to serve the needs of students who excel academically and artistically, according to the Department of Education. The schools are:\n\n* **Bronx High School of Science**\n\n* **The Brooklyn Latin School**\n\n* **Brooklyn Technical High School**\n\n* **High School of American Studies at Lehman College**, in the Bronx\n\n* **High School for Math, Science and Engineering at City College**, in Manhattan\n\n* **Queens High School for the Sciences at York College**\n\n* **Staten Island Technical High School**\n\n* **Stuyvesant High School**, in Manhattan\n\nThe ninth school, Fiorello H. LaGuardia High School of Music and Art and Performing Arts, on the Upper West Side,  uses student auditions and academic records, not  SHSAT scores, to determine admissions. So, in terms of our problem, we have **eight** specialized high schools.\n\nEach of the eight schools has a cutoff score, which is the minimum score that students must get to be offered a seat.\n\n## The problem\n\nNumbers don't lie. The number of black and latino students admitted to specialized high schools is **extremely** low. Only 10 percent of New York City’s public school students who are black or Latino received offers to attend a specialized high school last year, even though 67 percent of New York public school students are black or Latino. Asians make up 62 percent of students at specialized high schools and white students make up 24 percent, though only 16 percent of public school students are Asian and 15 percent are white.\n\nThe SHSAT helps determine who is most likely to enjoy success. The best way for the city to increase diversity at these schools is to improve the students’ primary school education and to provide greater access to quality, affordable test prep in advance of the SHSAT. This is the problem we have to solve."
    },
    {
      "metadata": {
        "_uuid": "43bbb29d4f8fd7522452bba3ab178950759c8500"
      },
      "cell_type": "markdown",
      "source": "As stated in the dataframe description, the **registrations** dataframe contains only data from district 5 (Central Harlem, Manhattan). Let's explore a bit more the dataframe."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83449d03bb94afaeb7aa43fa823a79efe84b5877",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "registrations.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cac46f287620571fbcef185b9793077e5c872b8e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "for i in registrations.columns:\n    print(i,',' ,'{:.1%}'.format(np.mean(registrations[i].isnull())),'nulls',',',type(registrations[i][0]), \n    registrations[i].nunique(), 'unique values')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "36811c487ee9ae8f03f353bfd93e4db7fb3e9e64",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Total number of students who registered for the SHSAT:',np.sum(registrations['Number of students who registered for the SHSAT']))\nprint('Total number of students who took the SHSAT:', np.sum(registrations['Number of students who took the SHSAT']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96c26b6a36a27d1bedf83bd6c45c3f4838729a2a"
      },
      "cell_type": "markdown",
      "source": "As opposed to the **df** dataframe, this one is clean! No null rows, so another thing we do not need to deal with. \n\nThere are two columns worth noticing: 'Number of students who took the SHSAT' and 'Number of students who registered for the SHSAT'. We will create a new column for the dataframe which is the percentage of students who registered for the SHSAT **and** took the exam."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09d0812ca1e0960e831112d0f3cd85bb16ca47f4",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "registrations['% of taken over registered students'] = (registrations['Number of students who took the SHSAT'])*100/(registrations['Number of students who registered for the SHSAT'])\n\nregistrations.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "70c501c70510fa85f2584926adaba6657b85d146"
      },
      "cell_type": "markdown",
      "source": "# *District codes in New York City*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a72f2de69488854acdd57de2f0dfc4cdb6411f79",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df['District'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06b64fabb59d281dfd0f405ce326b4ccc6b6ae19"
      },
      "cell_type": "markdown",
      "source": "\n\nThere are 32 district codes in the New York City:\n\n* District 1 to 6: Manhattan Borough\n* District 7 to 12: The Bronx Borough\n* District 13 to 23 and district 32: Brooklyn Borough\n* District 24 to 30: Queens Borough\n* District 31: Staten Island Borough\n\n![](http://)For those of us who are not familiar with the area, this helps quite a lot when it comes to understanding the data. \n\nThe district map looks like this:\n\n![](https://imageshack.com/a/img922/4818/Nl4VZg.jpg)"
    },
    {
      "metadata": {
        "_uuid": "953f0742f3d3d891c05f42f992c9a4a8045c2bb9"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "a9e5d526a610641e0f06f0ca6045786eb4fbe8e0"
      },
      "cell_type": "markdown",
      "source": "For convenience, I've created a column that specifies each of the different Boroughs, which makes it easier to understand."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "bfe001bd6bf8e60543ea9015d13ec63cfc10ba61"
      },
      "cell_type": "code",
      "source": "manh_districts = [1,2,3,4,5,6]\nbronx_districts = [7, 8,9,10,11,12]\nbrook_districts = [13,14,15,16,17,18,19,20,21,22,23,32]\nqueens_districts = [24,25,26,27,28,29,30]\nstaten_districts = [31]\n\n\ndf.loc[df['District'].isin(manh_districts), 'Borough'] = 'Manhattan'\ndf.loc[df['District'].isin(bronx_districts), 'Borough'] = 'Bronx'\ndf.loc[df['District'].isin(brook_districts), 'Borough'] = 'Brooklyn'\ndf.loc[df['District'].isin(queens_districts), 'Borough'] = 'Queens'\ndf.loc[df['District'].isin(staten_districts), 'Borough'] = 'Staten Island'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f34d345348b851014201b6a2c2ddba0ca862d656",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sns.pairplot(df[['Borough','Latitude','Longitude','Economic Need Index']], kind=\"scatter\", hue=\"Borough\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c958b5e12a302cda6ecbb2960a602351b4a18149",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df.loc[:,'Student Attendance Rate'] = df.loc[:,'Student Attendance Rate'] .str.replace('%', '')\ndf.loc[:,'Student Attendance Rate'] = df.loc[:,'Student Attendance Rate'] .astype(float)\nplt.figure(figsize=(15,10))\nsns.boxplot(x=\"Borough\", y=\"Student Attendance Rate\", data=df, dodge=False);\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3659eefe6956fb730613d4920217730395a1ffe8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df.loc[:,'Percent Black'] = df.loc[:,'Percent Black'].str.replace('%', '')\ndf.loc[:,'Percent Black'] = df.loc[:,'Percent Black'].astype(float)\ndf.loc[:,'Percent Hispanic'] = df.loc[:,'Percent Hispanic'].str.replace('%', '')\ndf.loc[:,'Percent Hispanic'] = df.loc[:,'Percent Hispanic'].astype(float)\ndf.loc[:,'Percent Asian']  = df.loc[:,'Percent Asian'].str.replace('%', '')\ndf.loc[:,'Percent Asian']  = df.loc[:,'Percent Asian'].astype(float)\ndf.loc[:,'Percent White'] = df.loc[:,'Percent White'] .str.replace('%', '')\ndf.loc[:,'Percent White'] = df.loc[:,'Percent White'] .astype(float)\n\n\nsns.lmplot('Economic Need Index', 'Percent Black', data=df, hue='Borough', fit_reg=True, size = 15)\nplt.title('Percent of Black students in the different Boroughs of NYC')\n\nsns.lmplot('Economic Need Index', 'Percent Hispanic', data=df, hue='Borough', fit_reg=True,  size = 15)\nplt.title('Percent of Hispanic students in the different Boroughs of NYC')\n\nsns.lmplot('Economic Need Index', 'Percent Asian', data=df, hue='Borough', fit_reg=True,  size = 15)\nplt.title('Percent of Asian students in the different Boroughs of NYC')\n\nsns.lmplot('Economic Need Index', 'Percent White', data=df, hue='Borough', fit_reg=True,  size = 15)\nplt.title('Percent of WHite students in the different Boroughs of NYC')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ee5744d573cb489c727da5a9c5a8bd0f812c284"
      },
      "cell_type": "markdown",
      "source": "To make sure we have relevant information from all 32 districts, we can plot the amount of inputs for each district."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b2b997169107356050759676cbb5b3bf3b0c197",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(10,10))\nax = sns.countplot(df['District'],label=\"Count\", order = df['District'].value_counts().index)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c4af7e3ec63e7161305a609738532c707145babf"
      },
      "cell_type": "markdown",
      "source": "We will now proceed to explore our dataset for each borough. To do so, we will divide our dataframe into 5 different dataframes, and calculate the median income for each of them (keeping in mind that we have not filled the missing values yet). The median is more relevant than the mean, since its the answer of the following question: what does the person earn who earns exactly more than 50% and less than 50% of the population? "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6eb47db528fc28d9fd41c5b1c3431a2cdb1b070",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_manha = df[(df['District'].isin(manh_districts))]\ndf_bronx = df[(df['District'].isin(bronx_districts))]\ndf_brook = df[(df['District'].isin(brook_districts))]\ndf_queens = df[(df['District'].isin(queens_districts))]\ndf_staten = df[(df['District'].isin(staten_districts))]\n\nmedian = df['School Income Estimate'].median()\nmedian_manha = df_manha['School Income Estimate'].median()\nmedian_bronx = df_bronx['School Income Estimate'].median()\nmedian_brook = df_brook['School Income Estimate'].median()\nmedian_queens = df_queens['School Income Estimate'].median()\nmedian_staten = df_staten['School Income Estimate'].median()\nprint('Global median:', median)\nprint('Median school income in Manhattan: $', median_manha)\nprint('Median school income in The Bronx: $', median_bronx)\nprint('Median school income in Brooklyn: $', median_brook)\nprint('Median school income in Queens: $', median_queens)\nprint('Median school income in Staten Island: $', median_staten)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ac4557dee3f2eafd4d3beb670019eb50b84256a"
      },
      "cell_type": "markdown",
      "source": "## Manhattan Borough\n\nAs we said, district codes 1, 2, 3, 4, 5 and 6 correspond to the Manhattan Borough. We will filter our dataframe with those districts and analyse the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f547662310deada9fb47dd42ed2262156552bd1a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import folium\nfrom folium import plugins\nfrom io import StringIO\nimport folium \n\ndf_manha['Economic Need Index'] = df_manha['Economic Need Index'].fillna((df_manha['Economic Need Index'].mean()))\n\n\n#colors = ['red', 'yellow', 'dusty purple', 'blue', 'white', 'brown', 'green', 'purple', 'orange', 'grey', 'coral']\ncolors = ['chartreuse', 'limegreen', 'yellowgreen', 'y', 'olive', 'indianred', 'firebrick', 'tomamto', 'orangered', 'red']\nd = (df_manha['Economic Need Index']*10).astype('int')\ncols = [colors[int(i)] for i in d]\n\n\nmap_osm2 = folium.Map([df_manha['Latitude'][0], df_manha['Longitude'][0]], zoom_start=10.2,tiles='cartodbdark_matter')\n\nfor lat, long, col in zip(df_manha['Latitude'], df_manha['Longitude'], cols):\n    #rown = list(rown)\n    folium.CircleMarker([lat, long], color=col, fill=True, radius=2).add_to(map_osm2)\n\nmap_osm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "734cd7e8fe40c5782b2299a5bc87455e3a08c690",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_bronx['Economic Need Index'] = df_bronx['Economic Need Index'].fillna((df_bronx['Economic Need Index'].mean()))\n\n\n#colors = ['red', 'yellow', 'dusty purple', 'blue', 'white', 'brown', 'green', 'purple', 'orange', 'grey', 'coral']\ncolors = ['chartreuse', 'limegreen', 'yellowgreen', 'y', 'olive', 'indianred', 'firebrick', 'tomamto', 'orangered', 'red']\nd = (df_bronx['Economic Need Index']*10).astype('int')\ncols = [colors[int(i)] for i in d]\n\n\nmap_osm2 = folium.Map([df_bronx['Latitude'].iloc[0], df_bronx['Longitude'].iloc[0]], zoom_start=10.2,tiles='cartodbdark_matter')\n\nfor lat, long, col in zip(df_bronx['Latitude'], df_bronx['Longitude'], cols):\n    #rown = list(rown)\n    folium.CircleMarker([lat, long], color=col, fill=True, radius=2).add_to(map_osm2)\n\nmap_osm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfdcf1926fb89e2ed6bcae091551787d84409d7f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_brook['Economic Need Index'] = df_brook['Economic Need Index'].fillna((df_brook['Economic Need Index'].mean()))\n\n\n#colors = ['red', 'yellow', 'dusty purple', 'blue', 'white', 'brown', 'green', 'purple', 'orange', 'grey', 'coral']\ncolors = ['chartreuse', 'limegreen', 'yellowgreen', 'y', 'olive', 'indianred', 'firebrick', 'tomamto', 'orangered', 'red']\nd = (df_brook['Economic Need Index']*10).astype('int')\ncols = [colors[int(i)] for i in d]\n\n\nmap_osm2 = folium.Map([df_brook['Latitude'].iloc[0], df_brook['Longitude'].iloc[0]], zoom_start=10.2,tiles='cartodbdark_matter')\n\nfor lat, long, col in zip(df_brook['Latitude'], df_brook['Longitude'], cols):\n    #rown = list(rown)\n    folium.CircleMarker([lat, long], color=col, fill=True, radius=2).add_to(map_osm2)\n\nmap_osm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "816652c869f47c9be6720baee072286928187c7d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_queens['Economic Need Index'] = df_queens['Economic Need Index'].fillna((df_queens['Economic Need Index'].mean()))\n\n\n#colors = ['red', 'yellow', 'dusty purple', 'blue', 'white', 'brown', 'green', 'purple', 'orange', 'grey', 'coral']\ncolors = ['chartreuse', 'limegreen', 'yellowgreen', 'y', 'olive', 'indianred', 'firebrick', 'tomamto', 'orangered', 'red']\nd = (df_queens['Economic Need Index']*10).astype('int')\ncols = [colors[int(i)] for i in d]\n\n\nmap_osm2 = folium.Map([df_queens['Latitude'].iloc[0], df_queens['Longitude'].iloc[0]], zoom_start=10.2,tiles='cartodbdark_matter')\n\nfor lat, long, col in zip(df_queens['Latitude'], df_queens['Longitude'], cols):\n    #rown = list(rown)\n    folium.CircleMarker([lat, long], color=col, fill=True, radius=2).add_to(map_osm2)\n\nmap_osm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ff3a4e3249213c8d649da858f73c59e69260640",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_staten['Economic Need Index'] = df_staten['Economic Need Index'].fillna((df_staten['Economic Need Index'].mean()))\n\n\n#colors = ['red', 'yellow', 'dusty purple', 'blue', 'white', 'brown', 'green', 'purple', 'orange', 'grey', 'coral']\ncolors = ['chartreuse', 'limegreen', 'yellowgreen', 'y', 'olive', 'indianred', 'firebrick', 'tomamto', 'orangered', 'red']\nd = (df_staten['Economic Need Index']*10).astype('int')\ncols = [colors[int(i)] for i in d]\n\n\nmap_osm2 = folium.Map([df_staten['Latitude'].iloc[0], df_staten['Longitude'].iloc[0]], zoom_start=10.2,tiles='cartodbdark_matter')\n\nfor lat, long, col in zip(df_staten['Latitude'], df_staten['Longitude'], cols):\n    #rown = list(rown)\n    folium.CircleMarker([lat, long], color=col, fill=True, radius=2).add_to(map_osm2)\n\nmap_osm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af8a7f141307196031fca2b1901ca94243a04103",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_manha.plot(kind=\"scatter\", x=\"Economic Need Index\", y=\"Percent Black\", figsize=(15,9) )\nplt.show()\ndf_manha.plot(kind=\"scatter\", x=\"Economic Need Index\", y=\"Percent Hispanic\", figsize=(15,9) )\nplt.show()\ndf_manha.plot(kind=\"scatter\", x=\"Economic Need Index\", y=\"Percent Asian\", figsize=(15,9) )\nplt.show()\ndf_manha.plot(kind=\"scatter\", x=\"Economic Need Index\", y=\"Percent White\", figsize=(15,9) )\nplt.show()\n\nplt.figure(figsize=(15,10))\n#ax = sns.violinplot(x=\"District\", y=\"Economic Need Index\", hue=\"Community School?\", data=df_manha, palette=\"muted\")\nsns.swarmplot(x=\"District\", y=\"Percent Black\", hue=\"Community School?\", data=df_manha)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a245497f2fd508d577dc2d94d7b61410f8f2554",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(df_manha.shape)\n\nax = sns.countplot(df_manha['Supportive Environment Rating'],label=\"Count\")       # M = 212, B = 357\nplt.show()\nax2 = sns.countplot(df_manha['Community School?'],label=\"Count\")       # M = 212, B = 357\nplt.show()\n\nprint(df_manha['Grades'].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3fe44430ccc4687e7c1c74e10d97e27917a552cd"
      },
      "cell_type": "markdown",
      "source": "Another thing worth exploring would be checking whether the amount of grades a school has is related with the amount of children that take the exam. To do so, we have to merge our two dataframes.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07f26f3953ccefd07c910f8a83dac8312be92c62",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(registrations['School name'].unique())\nprint(df['School Name'].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06e023524b8c93eeb59177930c0b4cb2aa452f63",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", c=\"Economic Need Index\", cmap=plt.get_cmap(\"jet\"),label='Schools', title='New York School Population Map',\n    colorbar=True, alpha=0.4, figsize=(15,9))\nplt.legend()\nplt.show()\n\ndf_manha.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", c=\"Economic Need Index\", cmap=plt.get_cmap(\"jet\"),label='Schools', title='New York School Population Map',\n    colorbar=True, alpha=0.4, figsize=(15,9) )\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(15,10))\n#ax = sns.violinplot(x=\"District\", y=\"Economic Need Index\", hue=\"Community School?\", data=df_manha, palette=\"muted\")\nsns.swarmplot(x=\"District\", y=\"Economic Need Index\", hue=\"Community School?\", data=df_manha)\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}